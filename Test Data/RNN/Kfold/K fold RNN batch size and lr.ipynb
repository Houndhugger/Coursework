{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "import numpy as np\n",
    "\n",
    "def seed_torch(seed=1029):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(RNNModel, self).__init__()\n",
    "        # Hidden dimensions\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Number of hidden layers\n",
    "        self.layer_dim = layer_dim\n",
    "\n",
    "        # Building your RNN\n",
    "        # batch_first=True causes input/output tensors to be of shape\n",
    "        # (batch_dim, seq_dim, feature_dim)\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, batch_first=True, nonlinearity='tanh')\n",
    "\n",
    "        # Readout layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # One time step\n",
    "        # We need to detach the hidden state to prevent exploding/vanishing gradients\n",
    "        # This is part of truncated backpropagation through time (BPTT)\n",
    "        out, hn = self.rnn(x, h0.detach())\n",
    "\n",
    "        # Index hidden state of last time step\n",
    "        # out.size() --> 100, 28, 100\n",
    "        # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! \n",
    "        out = self.fc(out) \n",
    "        # out.size() --> 100, 10\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import xlrd\n",
    "import pandas as pd\n",
    "\n",
    "book = xlrd.open_workbook('dataset for standardisation.xlsx')\n",
    "sheet = book.sheet_by_name('Sheet1')\n",
    "data_unreplicated = [[sheet.cell_value(r, c)\n",
    "         for c in range(1,9)] for r in range(2,74)]\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(data_unreplicated)\n",
    "\n",
    "\n",
    "book = xlrd.open_workbook('prepareddatakfold.xlsx')\n",
    "sheet = book.sheet_by_name('Sheet1')\n",
    "replicated_data = [[sheet.cell_value(r, c)\n",
    "         for c in range(0,8)] for r in range(0,7200)]\n",
    "\n",
    "def create_inout_sequences(input_data, tw):\n",
    "    training_seq = []\n",
    "    label_seq = []\n",
    "    L = len(input_data)\n",
    "    for i in range(0,L,tw):\n",
    "        train_seq = input_data[i:i+tw,0:5]\n",
    "        train_label = input_data[i:i+tw,5:8]\n",
    "        training_seq.append(train_seq)\n",
    "        label_seq.append(train_label)\n",
    "    return training_seq,label_seq\n",
    "\n",
    "standardised_data = scaler.transform(replicated_data)\n",
    "x = torch.tensor(standardised_data)\n",
    "\n",
    "z, t = create_inout_sequences(x, 12)\n",
    "x_train = torch.stack(z)\n",
    "y_train = torch.stack(t)\n",
    "n_splits = 6\n",
    "splits = list(KFold(n_splits=n_splits, shuffle=False, random_state=seed)\n",
    "              .split(x_train, y_train))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gabriel\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\Users\\Gabriel\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\Users\\Gabriel\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\Users\\Gabriel\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.0001 46.5 1.2483054410749015\n",
      "1 0.0002 30.0 1.2233866854508715\n",
      "1 0.0003 29.333333333333336 1.1951608682341046\n",
      "1 0.0004 3.6666666666666665 1.1888165075911412\n",
      "1 0.0005 8.666666666666668 1.1879972355895572\n",
      "1 0.0006 4.0 1.194553666975763\n",
      "1 0.0007 12.166666666666666 1.1937147261699042\n",
      "1 0.0008 22.499999999999996 1.2212794641653693\n",
      "1 0.0009 6.166666666666666 1.2668500853909386\n",
      "1 0.001 54.49999999999999 1.1956374606159001\n",
      "1 0.002 25.000000000000007 1.140637881424692\n",
      "1 0.003 17.833333333333336 1.174386572043101\n",
      "1 0.004 14.333333333333332 1.1000709764162697\n",
      "1 0.005 31.833333333333332 1.1778374572594958\n",
      "2 0.0001 64.83333333333333 1.2673262034522164\n",
      "2 0.0002 29.5 1.2593553339110481\n",
      "2 0.0003 39.5 1.205446089108785\n",
      "2 0.0004 34.66666666666667 1.2554481095737882\n",
      "2 0.0005 13.0 1.194572988483641\n",
      "2 0.0006 23.666666666666664 1.212956133418613\n",
      "2 0.0007 36.666666666666664 1.2219095942709182\n",
      "2 0.0008 26.666666666666668 1.1529667439725664\n",
      "2 0.0009 22.833333333333332 1.1581780086623297\n",
      "2 0.001 3.5 1.2303619078795114\n",
      "2 0.002 38.50000000000001 1.0800252514415316\n",
      "2 0.003 13.333333333333332 1.1036438217427995\n",
      "2 0.004 17.333333333333332 1.1294396029578313\n",
      "2 0.005 42.16666666666667 1.1112609140078227\n",
      "3 0.0001 21.333333333333332 1.2995711085531447\n",
      "3 0.0002 45.166666666666664 1.2675903623633917\n",
      "3 0.0003 29.5 1.2265221063296\n",
      "3 0.0004 34.5 1.1967812144756318\n",
      "3 0.0005 40.50000000000001 1.2024733984470368\n",
      "3 0.0006 23.166666666666664 1.2374323476685416\n",
      "3 0.0007 54.16666666666667 1.1495634877681733\n",
      "3 0.0008 22.5 1.1251890257994335\n",
      "3 0.0009 16.666666666666668 1.1838708033826617\n",
      "3 0.001 16.333333333333336 1.1896183595392438\n",
      "3 0.002 10.666666666666668 1.1698582109477786\n",
      "3 0.003 13.166666666666668 1.1728710079193116\n",
      "3 0.004 10.999999999999998 1.1340583176083034\n",
      "3 0.005 30.499999999999996 1.073793404367235\n",
      "4 0.0001 26.833333333333332 1.3086928153038024\n",
      "4 0.0002 39.5 1.2591171214315626\n",
      "4 0.0003 35.16666666666667 1.2514180223147076\n",
      "4 0.0004 32.166666666666664 1.236432263056437\n",
      "4 0.0005 18.833333333333332 1.245177298916711\n",
      "4 0.0006 18.333333333333332 1.1970108373959858\n",
      "4 0.0007 16.833333333333336 1.2061252885394627\n",
      "4 0.0008 9.666666666666668 1.2237424744500054\n",
      "4 0.0009 18.33333333333334 1.1992885865105523\n",
      "4 0.001 7.5 1.2136255497402615\n",
      "4 0.002 19.0 1.1719231912824843\n",
      "4 0.003 48.16666666666666 1.1499188857608371\n",
      "4 0.004 40.99999999999999 1.0173409785164726\n",
      "4 0.005 19.5 1.1808801974190606\n",
      "5 0.0001 30.333333333333332 1.3016193755467733\n",
      "5 0.0002 23.5 1.2890812985102333\n",
      "5 0.0003 49.66666666666667 1.2475467936197917\n",
      "5 0.0004 46.166666666666664 1.2257536967595417\n",
      "5 0.0005 27.333333333333336 1.2561426703135172\n",
      "5 0.0006 40.0 1.1820538483725653\n",
      "5 0.0007 23.333333333333336 1.273501074579027\n",
      "5 0.0008 13.666666666666668 1.25958512518141\n",
      "5 0.0009 13.833333333333332 1.2608008993996513\n",
      "5 0.001 4.333333333333333 1.2850982273949518\n",
      "5 0.002 28.0 1.1650645404391817\n",
      "5 0.003 13.5 1.2793822468651663\n",
      "5 0.004 22.666666666666664 1.1332816245820787\n",
      "5 0.005 25.833333333333336 1.2382619222005207\n",
      "6 0.0001 35.166666666666664 1.2949724663628475\n",
      "6 0.0002 66.0 1.287265468173557\n",
      "6 0.0003 20.0 1.2819043360816107\n",
      "6 0.0004 52.66666666666667 1.2624974266688027\n",
      "6 0.0005 27.333333333333336 1.2387209696239894\n",
      "6 0.0006 24.833333333333332 1.2351013416714138\n",
      "6 0.0007 18.666666666666668 1.189107724295722\n",
      "6 0.0008 40.5 1.2133975389268663\n",
      "6 0.0009 47.99999999999999 1.227876743475596\n",
      "6 0.001 17.166666666666668 1.280724843872918\n",
      "6 0.002 18.166666666666664 1.2246414036220974\n",
      "6 0.003 69.99999999999999 1.1321607128779094\n",
      "6 0.004 51.83333333333333 1.1286505360073513\n",
      "6 0.005 35.83333333333333 1.0488749326599969\n",
      "7 0.0001 48.333333333333336 1.3078572930230035\n",
      "7 0.0002 31.166666666666668 1.2951083453496297\n",
      "7 0.0003 25.5 1.296985772450765\n",
      "7 0.0004 18.166666666666664 1.2295983327759636\n",
      "7 0.0005 53.33333333333333 1.2475629546907212\n",
      "7 0.0006 11.166666666666668 1.2757297696007626\n",
      "7 0.0007 7.333333333333334 1.2879994882477654\n",
      "7 0.0008 35.5 1.1595732254452176\n",
      "7 0.0009 18.0 1.211164100964864\n",
      "7 0.001 45.33333333333334 1.1458391425344678\n",
      "7 0.002 4.333333333333334 1.2552909236484104\n",
      "7 0.003 23.166666666666668 1.249268806245592\n",
      "7 0.004 57.666666666666664 1.1697669821315342\n",
      "7 0.005 23.166666666666664 1.1850219239128958\n",
      "8 0.0001 55.833333333333336 1.3085336833530001\n",
      "8 0.0002 32.5 1.2876267194747923\n",
      "8 0.0003 21.333333333333336 1.2677905294630263\n",
      "8 0.0004 37.5 1.2548658328586155\n",
      "8 0.0005 13.666666666666668 1.279497848086887\n",
      "8 0.0006 8.333333333333334 1.241289840274387\n",
      "8 0.0007 25.999999999999996 1.2782922744750977\n",
      "8 0.0008 46.33333333333333 1.221463836034139\n",
      "8 0.0009 22.333333333333332 1.2528673013051352\n",
      "8 0.001 31.000000000000004 1.2070825306574504\n",
      "8 0.002 25.0 1.2580048651165432\n",
      "8 0.003 17.5 1.2147250609927709\n",
      "8 0.004 29.333333333333332 1.2208243197864956\n",
      "8 0.005 63.666666666666664 1.1936089324951171\n",
      "9 0.0001 49.33333333333333 1.30223124994172\n",
      "9 0.0002 25.0 1.30151186770863\n",
      "9 0.0003 82.5 1.2469939333862727\n",
      "9 0.0004 47.5 1.2816541683673859\n",
      "9 0.0005 63.5 1.2636297452449798\n",
      "9 0.0006 30.0 1.2440199234750535\n",
      "9 0.0007 51.16666666666667 1.2206894854042265\n",
      "9 0.0008 39.5 1.2119784277015262\n",
      "9 0.0009 34.5 1.252675991455714\n",
      "9 0.001 43.833333333333336 1.192761761016316\n",
      "9 0.002 31.833333333333336 1.172859408458074\n",
      "9 0.003 18.833333333333332 1.1743410377369985\n",
      "9 0.004 37.0 1.1335072537263233\n",
      "9 0.005 62.0 1.2429870675669776\n",
      "10 0.0001 53.50000000000001 1.3062948905097116\n",
      "10 0.0002 34.33333333333333 1.2990770848592124\n",
      "10 0.0003 26.333333333333332 1.252603021197849\n",
      "10 0.0004 61.166666666666664 1.2466395812564426\n",
      "10 0.0005 12.166666666666668 1.2678226025899253\n",
      "10 0.0006 14.5 1.229331191380819\n",
      "10 0.0007 12.666666666666666 1.2480678314632838\n",
      "10 0.0008 10.833333333333332 1.2247094016604954\n",
      "10 0.0009 11.0 1.2436412949032252\n",
      "10 0.001 7.666666666666667 1.230431991153293\n",
      "10 0.002 10.666666666666668 1.2719012175665962\n",
      "10 0.003 20.333333333333332 1.2263014147016738\n",
      "10 0.004 28.333333333333336 1.215985296037462\n",
      "10 0.005 22.0 1.3201247787475587\n",
      "20 0.0001 98.0 1.3059241167704263\n",
      "20 0.0002 52.166666666666664 1.299166734483507\n",
      "20 0.0003 36.333333333333336 1.2845233514573837\n",
      "20 0.0004 69.66666666666667 1.2619886673821343\n",
      "20 0.0005 19.833333333333336 1.2877814971076118\n",
      "20 0.0006 22.0 1.2669761594136555\n",
      "20 0.0007 18.5 1.2342687288920084\n",
      "20 0.0008 63.0 1.190375453101264\n",
      "20 0.0009 16.833333333333336 1.2050288709004722\n",
      "20 0.001 12.499999999999998 1.2370879703097872\n",
      "20 0.002 29.833333333333332 1.2089746930864123\n",
      "20 0.003 6.5 1.3147178819444445\n",
      "20 0.004 19.833333333333332 1.2661415015326607\n",
      "20 0.005 20.0 1.2219331169128418\n",
      "40 0.0001 135.49999999999997 1.6329406272040474\n",
      "40 0.0002 90.49999999999999 1.2980300267537435\n",
      "40 0.0003 63.833333333333336 1.2943390782674151\n",
      "40 0.0004 50.83333333333333 1.2797096337212457\n",
      "40 0.0005 39.5 1.2785254245334203\n",
      "40 0.0006 41.33333333333333 1.2419017219543456\n",
      "40 0.0007 27.333333333333332 1.2650838067796497\n",
      "40 0.0008 27.666666666666668 1.2619141345553926\n",
      "40 0.0009 28.666666666666668 1.2144387796190048\n",
      "40 0.001 28.0 1.195946373409695\n",
      "40 0.002 16.666666666666668 1.206635971069336\n",
      "40 0.003 48.83333333333334 1.3184336789449056\n",
      "40 0.004 51.99999999999999 1.1986657990349665\n",
      "40 0.005 62.00000000000001 1.2951348601447212\n",
      "60 0.0001 149.16666666666666 1.6928841696845163\n",
      "60 0.0002 100.16666666666666 1.6428437466091579\n",
      "60 0.0003 80.33333333333333 1.303718032836914\n",
      "60 0.0004 86.5 1.2497004742092555\n",
      "60 0.0005 50.833333333333336 1.2817571004231771\n",
      "60 0.0006 41.0 1.2804042180379231\n",
      "60 0.0007 41.0 1.2491766357421874\n",
      "60 0.0008 40.5 1.2300572670830618\n",
      "60 0.0009 41.166666666666664 1.2113615968492297\n",
      "60 0.001 42.666666666666664 1.1873277960883248\n",
      "60 0.002 64.83333333333333 1.1346066856384276\n",
      "60 0.003 15.166666666666668 1.2782618798149956\n",
      "60 0.004 49.5 1.2674330732557508\n",
      "60 0.005 81.83333333333333 1.191990466647678\n",
      "60 0.0001 148.16666666666666 1.6918534172905817\n",
      "60 0.0002 97.83333333333333 1.6477281782362196\n",
      "60 0.0003 91.5 1.2984009255303277\n",
      "60 0.0004 92.83333333333334 1.261603732638889\n",
      "60 0.0005 49.5 1.2818938827514648\n",
      "60 0.0006 43.49999999999999 1.2759784825642904\n",
      "60 0.0007 44.16666666666667 1.272438693576389\n",
      "60 0.0008 32.166666666666664 1.2609782282511393\n",
      "60 0.0009 32.5 1.265971467759874\n",
      "60 0.001 38.333333333333336 1.2000847371419272\n",
      "60 0.002 26.666666666666664 1.2004232660929361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 0.003 13.666666666666666 1.2853498119778104\n",
      "60 0.004 22.333333333333336 1.215313991970486\n",
      "60 0.005 55.833333333333336 1.2183089065551758\n",
      "100 0.0001 167.0 1.7598644341362848\n",
      "100 0.0002 142.99999999999997 1.6668384467230903\n",
      "100 0.0003 121.33333333333334 1.3494934421115452\n",
      "100 0.0004 74.00000000000001 1.656617702907986\n",
      "100 0.0005 87.5 1.2860209062364365\n",
      "100 0.0006 75.16666666666666 1.274664518568251\n",
      "100 0.0007 62.50000000000001 1.2724451276991104\n",
      "100 0.0008 59.666666666666664 1.263226538764106\n",
      "100 0.0009 54.33333333333333 1.2561819458007812\n",
      "100 0.001 53.833333333333336 1.2596838972303603\n",
      "100 0.002 43.5 1.1825265333387587\n",
      "100 0.003 27.166666666666664 1.2350016530354817\n",
      "100 0.004 19.166666666666668 1.2692424096001518\n",
      "100 0.005 39.5 1.29018805609809\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "i = n_splits-1\n",
    "test_batchsizes = [1,2,3,4,5,6,7,8,9,10,20,40,60,60,100]\n",
    "test_lrs = [0.0001,0.0002,0.0003,0.0004,0.0005,0.0006,0.0007,0.0008,0.0009,\n",
    "            0.001,0.002,0.003,0.004,0.005]\n",
    "input_dim = 5\n",
    "hidden_dim = 25\n",
    "layer_dim = 2\n",
    "output_dim = 3\n",
    "model = RNNModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "init_state = copy.deepcopy(model.state_dict())\n",
    "for batchsize in test_batchsizes:\n",
    "    for test_lr in test_lrs:\n",
    "            loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "            learning_rate = test_lr\n",
    "            av_opt_epoch=0\n",
    "            av_opt_loss=0\n",
    "            for i, (train_idx, valid_idx) in enumerate(splits):\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "                model.load_state_dict(init_state)\n",
    "                x_train_fold = torch.tensor(x_train[train_idx], dtype=torch.float32)\n",
    "                y_train_fold = torch.tensor(y_train[train_idx], dtype=torch.float32)\n",
    "                x_val_fold = torch.tensor(x_train[valid_idx], dtype=torch.float32)\n",
    "                y_val_fold = torch.tensor(y_train[valid_idx], dtype=torch.float32)\n",
    "                train = torch.utils.data.TensorDataset(x_train_fold, y_train_fold)\n",
    "                valid = torch.utils.data.TensorDataset(x_val_fold, y_val_fold)\n",
    "                train_loader = torch.utils.data.DataLoader(train, batch_size=batchsize,\n",
    "                                                           shuffle=True)\n",
    "                valid_loader = torch.utils.data.DataLoader(valid, batch_size=batchsize,\n",
    "                                                           shuffle=False)\n",
    "                UP=0\n",
    "                epoch=0\n",
    "                k=0\n",
    "                opt_loss=1000\n",
    "                old_avg_val_loss=0\n",
    "                while UP<4 and epoch<300:\n",
    "                    epoch+=1\n",
    "                    k+=1\n",
    "                    model.train()\n",
    "                    for x_batch, y_batch in train_loader:\n",
    "                        y_pred = model(x_batch)\n",
    "                        loss = loss_fn(y_pred, y_batch)\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                    model.eval()\n",
    "                    avg_val_loss = 0.\n",
    "                    for x_batch, y_batch in valid_loader:\n",
    "                        y_pred = model(x_batch).detach()\n",
    "                        avg_val_loss += (loss_fn(y_pred, y_batch).item() / \n",
    "                        (len(valid_loader.dataset)*x_batch.size(1)))\n",
    "                    if k > 4:\n",
    "                        if avg_val_loss> old_avg_val_loss:\n",
    "                            UP+=1\n",
    "                        if avg_val_loss<old_avg_val_loss:\n",
    "                            UP=0\n",
    "                        old_avg_val_loss=avg_val_loss\n",
    "                        k=0\n",
    "                    if avg_val_loss<opt_loss:\n",
    "                        opt_loss = avg_val_loss\n",
    "                        opt_epoch = epoch\n",
    "                \n",
    "                av_opt_epoch+=opt_epoch/n_splits\n",
    "                av_opt_loss+=opt_loss/n_splits\n",
    "            print(batchsize,test_lr,av_opt_epoch,av_opt_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), (\"C:/Users/Gabriel/Documents/Python/ANN3.pt\"))                \n",
    "                                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
