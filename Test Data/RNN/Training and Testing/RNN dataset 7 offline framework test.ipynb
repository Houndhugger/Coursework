{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "# Define the model\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, batch_first=True, nonlinearity='tanh')\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        out, hn = self.rnn(x, h0.detach())\n",
    "        out = self.fc(out) \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define architecture\n",
    "input_dim = 5\n",
    "hidden_dim = 15\n",
    "layer_dim = 1 \n",
    "output_dim = 3\n",
    "\n",
    "model = RNNModel(input_dim, hidden_dim, layer_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlrd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#load trained model \n",
    "model.load_state_dict(torch.load(\"C:/Users/Gabriel/Documents/Python/RNN1.pt\"))\n",
    "model.eval()\n",
    "\n",
    "#load dataset used for standardisation\n",
    "book = xlrd.open_workbook('dataset for standardisation.xlsx')\n",
    "sheet = book.sheet_by_name('Sheet1')\n",
    "data4 = [[sheet.cell_value(r, c)\n",
    "         for c in range(1,6)] for r in range(2,74)]\n",
    "data5 = [[sheet.cell_value(r, c)\n",
    "         for c in range(6,9)] for r in range(2,74)]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler2 = StandardScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset 7\n",
    "book = xlrd.open_workbook('Experimental Data edited.xlsx')\n",
    "sheet = book.sheet_by_name('Sheet2')\n",
    "#choose the number of timesteps the model will 'see' before predicting the rest \n",
    "points_given=1\n",
    "data = [[sheet.cell_value(r, c)\n",
    "         for c in range(1,6)] for r in range(114,114+points_given)]\n",
    "data3 = [[sheet.cell_value(r, c)\n",
    "         for c in range(1,4)] for r in range(115,127)]\n",
    "data2 = [[sheet.cell_value(r, c)\n",
    "         for c in range(1,6)] for r in range(114,126)]\n",
    "actual_results = torch.tensor(data3)\n",
    "\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "scaler.fit(data4)\n",
    "scaler2.fit(data5)\n",
    "\n",
    "#sequence the data\n",
    "test_data = torch.tensor(data)\n",
    "test_data_all = torch.tensor(data2)\n",
    "sequenced_test_data_all = test_data_all.view(1,12,5)\n",
    "sequenced_test_data = test_data.view(1,points_given,5)\n",
    "\n",
    "\n",
    "for i in range (12-points_given):\n",
    "    #standardise the data\n",
    "    unsequenced_test_data = sequenced_test_data.view(i+points_given,5)\n",
    "    scaled_test_data = torch.tensor(scaler.transform(unsequenced_test_data))\n",
    "    sequenced_scaled_test_data = scaled_test_data.view(1,i+points_given,5)\n",
    "    with torch.no_grad():\n",
    "        #predict the rate of change\n",
    "         data_pred = model(sequenced_scaled_test_data.float())\n",
    "    predictions = np.array(data_pred)\n",
    "    #destandardise the rate of change \n",
    "    descaled_predictions = scaler2.inverse_transform(predictions)\n",
    "    usable_predictions = torch.tensor(descaled_predictions)\n",
    "    #add the rate of change to the previous timestep for prediction of the next timestep\n",
    "    true_predictions = (sequenced_test_data[:,(i+points_given-1):(i+points_given),0:3] +\n",
    "    (usable_predictions[:,(i+points_given-1):(i+points_given),:]*12))\n",
    "    tensor_predictions = torch.tensor(true_predictions.float())\n",
    "    #append prediction of the next timestep to the bottom of the current sequence\n",
    "    prediction_added = torch.cat((sequenced_test_data.float()[:,:,0:3],\n",
    "                              tensor_predictions[:,:,:]),1)\n",
    "    #add light intensity and nitrate inflow conc data to the new timestep\n",
    "    sequenced_test_data = torch.cat((prediction_added,\n",
    "                               sequenced_test_data_all.float()\n",
    "                                     [:,0:(points_given+1)+i,3:5]),2)\n",
    "    final_predictions = sequenced_test_data\n",
    "\n",
    "x_plot = final_predictions.view(12,5)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#calculate MAPE and generate graphs\n",
    "def MAPE(y_true, y_pred):\n",
    "    MAPE_Total=0\n",
    "    for i in range(len(y_pred)):\n",
    "        MAPE = torch.abs((y_true[i:1+i,:]-y_pred[i:1+i,:])/y_true[i:1+i,:])\n",
    "        MAPE_Total += (MAPE/len(y_pred))*100\n",
    "    return MAPE_Total\n",
    "\n",
    "import numpy as np\n",
    "a = np.linspace(12,144,12)\n",
    "v = torch.tensor(data3)\n",
    "import matplotlib.pyplot as plt \n",
    "y1 = x_plot[:, 0:1] \n",
    "x1 = a\n",
    "plt.plot(x1, y1,'o' ,label = \"predicted\") \n",
    "\n",
    "y2 = v[:, 0:1] \n",
    "x2 = a\n",
    "plt.plot(x2, y2, 'o',label = \"experimental\") \n",
    "plt.xlabel('time (h)')\n",
    "plt.ylabel('Biomass Conc (g/L)')\n",
    "loss = MAPE(v[:, 0:1],x_plot[:, 0:1])\n",
    "plt.title(('MAPE = {:.2f}'.format(loss.item())))\n",
    "plt.grid(b=True,which='major', axis='both')\n",
    "plt.legend()\n",
    "plt.show() \n",
    "\n",
    "\n",
    "y1 = x_plot[:, 1:2] \n",
    "x1 = a\n",
    "plt.plot(x1, y1,'o',label = \"predicted\") \n",
    "\n",
    "y2 = v[:, 1:2] \n",
    "x2 = a\n",
    "plt.plot(x2, y2,'o',label = \"experimental\") \n",
    "plt.xlabel('time (h)')\n",
    "plt.ylabel('Nitrate Conc (mg/L)')\n",
    "loss = MAPE(v[:, 1:2],x_plot[:, 1:2])\n",
    "plt.title(('MAPE = {:.2f}'.format(loss.item())))\n",
    "plt.grid(b=True,which='major', axis='both')\n",
    "plt.legend()\n",
    "plt.show() \n",
    "\n",
    "y1 = x_plot[:, 2:3] \n",
    "x1 = a\n",
    "plt.plot(x1, y1,'o',label = \"predicted\") \n",
    "\n",
    "y2 = v[:, 2:3] \n",
    "x2 = a\n",
    "plt.plot(x2, y2,'o',label = \"experimental\") \n",
    "plt.xlabel('time (h)')\n",
    "plt.ylabel('Lutein Conc (mg/L)')\n",
    "loss = MAPE(v[:, 2:3],x_plot[:, 2:3])\n",
    "plt.title(('MAPE = {:.2f}'.format(loss.item())))\n",
    "plt.grid(b=True,which='major', axis='both')\n",
    "plt.legend()\n",
    "plt.show() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export results to excel\n",
    "import pandas as pd\n",
    "b=np.array(v)\n",
    "c=np.array(x_plot)\n",
    "book = xlrd.open_workbook('tested data.xlsx')\n",
    "sheet = book.sheet_by_name('Sheet1')\n",
    "writer = pd.ExcelWriter('tested data.xlsx', engine='xlsxwriter')\n",
    "df1 = pd.DataFrame(b)\n",
    "df1.to_excel(writer, header=True, index=True)\n",
    "df2 = pd.DataFrame(c)\n",
    "df2.to_excel(writer, startrow=13, header=True, index=True)\n",
    "writer.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
